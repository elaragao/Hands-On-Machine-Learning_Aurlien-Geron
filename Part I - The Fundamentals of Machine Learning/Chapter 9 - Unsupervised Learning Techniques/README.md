# Unsupervised Learning Techniques

Although most of the efforts made for Machine Learning are in supervised learning, most of the data that exists does not have captions, which highlights the importance of **unsupervised learning**. The following topics will be covered in the following chapter:

- **Clustering**: Groups similar instances into _Clusters_. It is useful for data analysis, customer segmentation, recommendation systems, search systems, image sizing, etc.

- **Anomaly Detection (or Outlier Detection)**: Its objective is to identify what is "normal" for the system and detect abnormal instances. Instances considered normal are called **inliers** and those that are abnormal are called **anomalies** or **outliers**. Widely used in fraud detection, defective products, time series analysis, etc.

- **Density Estimation**: Estimates the Probability Density Function (PDF) of what generated the data set. Used for anomaly detection such as instances in very low density regions, and also useful for data visualization.
  
<!---------------------------------------------------->
<!---------------------------------------------------->
<!---------------------------------------------------->


# Clustering Algorithms: k-means and DBSCAN 

**Clustering** consists of grouping instances within a group with other similar instances, called a **Cluster**. It is an unsupervised process, different from classification. Some of the applications of **Clustering** are:

- **Customer Segmentation**: Useful for _Recommender Systems_, when identifying patterns of purchases and activities on websites.

- **Data Analysis**: Helps in visualization;

- **Dimensionality Reduction**: After clustering, it is possible to see the _Affinity_ of the instances, which measures how well an instance is in a cluster.

- **Feature Engineering**: As shown in chapter 2, the geographic cluster of the California Housing dataset.

- **Anomaly Detection**: Detects based on the _Low Affinity_ of the instances in the clusters.

- **Semi-Supervised Learning**: If there are some instances, it is possible to cluster and propagate the captions to other instances in the same cluster.

- **Search Engines**: An example is image searches, finding one that is close to the cluster of the image used to perform the search.

- **Image Segmentation**: Clusters pixels according to colors, replacing and reducing the number of different colors, used for object detection and tracking systems, in addition to detecting contours.

There is no single way to define a cluster, it depends on the context and the algorithm. Some search for the center around a defined point called _Centroid_, others search for denser regions. Two algorithms for this called **k-means** and **DBSCAN** will be discussed shortly.
<!---------------------------------------------------->
<!---------------------------------------------------->
<!---------------------------------------------------->


## k-means 

The **k-means** algorithm is simple and capable of clustering certain data sets quickly, efficiently and in a few iterations. The algorithm is also known as the Lloyd-Forgy algorithm.


[Image]


The Scikit-Learn library through the `KMeans` class. The algorithm requires that you specify the number of clusters that will be analyzed (in general, this is not a trivial task, but it will be discussed below). For each instance that was identified as belonging to a cluster, a label will be assigned, corresponding to the index. Both the indices can be seen through the `labels_instance` variable, and the centroids through `cluster_centers_`.


```python
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
X, y = make_blobs([...]) # make the blobs: y contains the cluster IDs, but we
# will not use them; that's what we want to predict

k = 5
kmeans = KMeans(n_clusters=k, random_state=42)
y_pred = kmeans.fit_predict(X)

y_pred
y_pred is kmeans.labels_

kmeans.cluster_centers_
```


>[!CAUTION]
> The labels generated by the clustering algorithm are **different** from the labels in classification classes, which are used as **targets**. Clustering is an **Unsupervised** learning activity.


The algorithm does not perform as well when the clusters have different diameters, since it operates through the distance between the instance and the centroid. This is observable in some points identified as in the pink cluster, which would belong to the yellow cluster, as in the image below:

[Image]

In this algorithm, there are two main ways to classify the instances:
- **Hard Clustering**: Allocates the instance in the cluster closest to the centroid;
- **Soft Clustering**: Assigns scores to each instance in relation to each cluster. This score can be in relation to the instance and centroid, or in relation to the **Similarity Score** or **Affinity* (for example, Gaussina Radial Basis Function).
<!---------------------------------------------------->

### The k-means algorithm

In a considerable number of cases, it is not certain how many clusters there are. The algorithm can be operated by randomly placing the centroids in k locations, for example, in k random instances. Then, the instances are labeled and the centroids are updated, and this is done repeatedly. The image below demonstrates this starting in the top left corner, followed by the top right corner where the instances are labeled.

[Image]

>[!NOTE]
> The algorithm converges in a small number of iterations, usually small, since the mean squared distance of the instances and centroids decreases at each step and converges.

Although convergence is certain, it may not converge to the correct solution, but rather to a _local optimum_, depending on the initialization of the centroid.


<!---------------------------------------------------->

### Centroid initialization methods

One way to initialize the centroids in a more optimized way is to use the data found in the random initialization. This can be done as in the code below:

```python
good_init = np.array([[-3, 3], [-3, 2], [-3, 1], [-1, 2], [0, 2]])
kmeans = KMeans(n_clusters=5, init=good_init, n_init=1, random_state=42)
kmeans.fit(X)
```

Another possible solution is to run the algorithm several times and keep the best solution. This can be done using the hyperparameter `n_init` as any integer number you want.



> [!IMPORTANT]
> The way to identify which solution is the best is through the _Performance Metric_ called **Inertia**, which is the sum of the distances calculated between the instances and the closest centroid. The model maintained is the one with the lowest **Inertia**. It is observable through `kmeans.inertia_`. It is worth noting that the `score` method is its negative.


Recently, an improvement was made to the algorithm, called **k-means++**. It adds a computational step that drastically reduces the number of iterations that need to be done to find the optimal solution, and reduces the chance of falling into local minima or suboptimal solutions. It chooses to select centroids that are **Far** from each other. The `KMeans` class already uses this method.

The algorithm operates in the steps of, first, selecting the centroid $c^{(1)}$ in the dataset. After that, select a new centroid $c^{(i)}$ by choosing an instance $x^{(1)}$ through the probability expressed by the equation $D (x^{(i)})^{2} / \sum^{m}_{j=1}D (x^{(j)})^{2}$, where $D (x^{(i)})$ is the distance between the instance $x^{(i)}$ and the closest centroid already chosen (in the case in question, it would be $c^{(1)}$) and repeat this step until selecting the k centroids.



<!---------------------------------------------------->
### Accelerated k-means and mini-batch k-means

The **Elkan Algorithm** or **Accelerated Algorithm** is used in data sets with many clusters, avoiding unnecessary distance calculations. It does this through the triangle inequality and controlling lower and upper bounds for the distances between instances and centroids. It is used through the hyperparameter `algorithm = "elkan"`.

The use of **mini-batches** proposed by Schulley uses mini-batches by moving centroids only a little at each iteration, speeding up the algorithm and making it possible to cluster large data sets. It is possible to use it with the `MiniBatchKMeans` class

>[!TIP]
> In cases where the data set does not fit in memory, there are a few options: use the `memmap` class, use _mini-batch_ together with the `partial_fit()` method, although it will require much more work.

>[!CAUTION]
> The algorithm using **Mini-Batches** is much faster than k-means, but has a slightly worse _Inertia_.






<!---------------------------------------------------->
### Finding the optimal number of clusters

In some cases, it may seem obvious how many clusters are needed to start the algorithm. For data where there are none, it is common to consider the **lowest Inertia value** as discrimination, but the more centroids, the lower the inertia value, so it is **not a good parameter**.

Another way to analyze would be to see the inertia as a function of $k$, at the inflection point called _elbow_, but using as an example the case where we know that the ideal number is 5, the inflection point points to $k = 4$.

[Image]

Another technique to identify the best value is somewhat crude and computationally expensive. The technique consists of using **Sillhouette Score** acquired from the average **Sillhouette Coefficient** of the instances. The score can be observed by the `silhouette_score` class. The equation below shows how the coefficient is obtained:

```math
Sillhouete \: \: Coefficient \: \: = \frac{(b-a)}{max(a,b)}
```
<details>

<summary>Equation terms</summary>

- $a$: This is the average intra-cluster distance, or average distance for instances within the same cluster;

- $b$: This is the average distance from the closest cluster (minimum distance between the closest instances from different clusters);

</details>

The coefficient can vary between -1 and +1, where:

- **Close to +1**: Instance is well allocated within the cluster and far from the others;

- **Close to 0**: Instance close to a cluster boundary;

- **Close to -1**: Instance possibly assigned to the wrong instance.

One way to visualize this is by plotting the Silhouette Score as a function of k, but in addition, for better visualization, you can plot the **Silhouette Diagram** graph. The graphs are blade-shaped, where the **height** of the shape indicates the number of instances in this cluster and the **width** represents the average of the coefficients of the instances in the cluster (wider = better).

[Image]

The vertical line represents the average score of the silhouette. If the instances are to the left of the line, it indicates that there was poor clustering.
<!---------------------------------------------------->
<!---------------------------------------------------->
## Limits of k-means

Although it has several advantages, the k-means algorithm performs poorly when faced with clusters of different sizes, densities or shapes that are not spherical. An example would be the **Gaussian Mixture** algorithm, which performs better on elliptical shapes.

>[!TIP]
> It is important to emphasize that **Scaling** the inputs is important before applying the algorithms.

<!---------------------------------------------------->
<!---------------------------------------------------->
                           
## Using Clustering for Image Segmentation 


**Image Segmentation** partitions an image into different segments, and has some variations such as:

- **Color Segmentation**: Pixels with the same color are assigned to the same segment. Useful in satellite forest preservation analysis;
- **Semantic Segmentation**: Pixels that are part of the same object are in the same segment. Useful in cars driving to identify signs or pedestrians.
- **Instance Segmentation**: Similar to the previous one, but more specific. It would identify each sign as one and each pedestrian as one.

The last two segmentations are more complex and tend to be used with convolutional neural networks, so the first one will be addressed now. The image `ladybug.png` from the `Pillow` library will be used as an example. The image is a 3D array, where the first dimension is the height, the second the width (number of pixels in the rows and columns) and the third the intensity of the RGB colors. In short, each pixel contains an RGB representation from 0 to 255. Additional channels could represent transparency, or infrared for satellites.

It is possible to reshape the array to obtain a list of RGB colors. The code below demonstrates this being used and applied to 8 clusters. It is important to note that the smaller the number of clusters, the smaller details of the image will go unnoticed by the algorithm.

```python
X = image.reshape(-1, 3)
kmeans = KMeans(n_clusters=8, random_state=42).fit(X)
segmented_img = kmeans.cluster_centers_[kmeans.labels_]
segmented_img = segmented_img.reshape(image.shape)
```

<!---------------------------------------------------->
<!---------------------------------------------------->

## Using Clustering for Semi-Supervised Learning  

Another use would be for data sets with only a few labels, for semi-supervised learning. Using the MNIST numerical set as an example.

To demonstrate this, it is possible to assume that we only have the labels of 50 of these, and use them to obtain the caption of the others through clustering.

<!---------------------------------------------------->
<!---------------------------------------------------->

## DBSCAN 

**Density Based Spatial Clustering of Applications with Noise** (**DBSCAN**) is an algorithm that defines clusters with a continuous region of high density. The way the algorithm operates is as follows:

- Step 1: It uses a small distance $\varepsilon$ to see how many instances are close together in the region, called **$\varepsilon$-neighborhood**;
- Step 2: If there is a minimum number of instances in the $\varepsilon$-neighborhood, there are regions considered dense, called **Core Instance**
- Step 3: A set of **Core Instances** forms a **Cluster**.
- Step 4: Instances outside these sets are considered **Anomalies**.

The algorithm is useful for separating clusters from low-density regions. An example of the code can be seen below with the `DBSCAN` class:

```python
from sklearn.cluster import DBSCAN
from sklearn.datasets import make_moons

X, y = make_moons(n_samples=1000, noise=0.05)
dbscan = DBSCAN(eps=0.05, min_samples=5)
dbscan.fit(X)
```

When a label detects a cluster index as -1 it means that the instance is an anomaly. It is possible to visualize both the _Core Instances_ and their indices with `components_` and `core_sample_indices_`.

The `DBSCAN` class does not have a `predict()` method, only the `fit_predict()` method, being unable to predict which cluster new instances belong to. This is not a problem because there are several other better methods for this task, such as `KNeighborsClassifier`.

This allows you to use the entire training set when training a classifier with only the _Core Instances_, ignore anomalies, or select only the anomalies.

**DBSCAN** allows you to identify any number of clusters in any format. It is important to note that **it does not perform well with large datasets** and has problems with **low density** regions.


<!---------------------------------------------------->
<!---------------------------------------------------->
                                        
## Other Clustering Algorithms   

Some other algorithms from the Scikit-Learn library are:

- **Agglomerative Clustering**: Hierarchical clustering from the bottom up; it operates from individual sets, and gradually clusters them until a single large cluster is formed. It has a tree-like format. It is useful for clusters of different formats and is flexible in performance, scaling well with large datasets if there is a _connectivity matrix_ (sparse m × m matrix indicating that pairs of instances are neighbors).

- **BIRCH**: Or **Balanced Iterative reducing and Clustering using Hierarchies**, it performs well with large datasets and with a not very large number of features (<20). It can be faster than _batch k-mean_.

- **Mean-Shift**: In this algorithm, a point called _Particle_ is initially defined in each instance, and the local density in the data space is calculated. Then the particle is moved to where the density is concentrated and continues to move until the local intensity peak is reached, forming a cluster. It is capable of forming clusters with different numbers and shapes, uses only one hyperparameter and depends on the local density. It scales poorly with large datasets.

- **Affinity Propagation**: In this algorithm, instances choose another instance (or themselves) to "represent" them, and each instance chosen for this purpose is called _Exemplar_. Each _Exemplar_ and its "voters" form a cluster. It is similar to _K-Mean_, but does not require the number of clusters to be chosen beforehand, operates well with clusters of different sizes and scales poorly with a large dataset.

- **Spectral Clustering**: Explaining it in a simpler way, it uses a similarity matrix between the instances, reduces the dimensionality of the matrix and uses a clustering algorithm on this similarity matrix with reduced dimensionality. It does not work well with clusters of different sizes, but it handles different and irregular shapes well, being able to identify more complex patterns, such as clusters of friends in social networks. It does not scale well with large datasets.

<!---------------------------------------------------->
<!---------------------------------------------------->
<!---------------------------------------------------->


# Gaussian Mixtures

**Gaussian Mixtures Model** (GMM) assumes that each instance was generated using a series of Gaussian distributions with unknown parameters, and that those generated by a common Gaussian distribution are part of the same ellipsoidal cluster, but with different shapes (still ellipsoidal), sizes, orientations, and densities.

There are many variants of GMM, the simplest of which requires knowing the number of k Gaussian distributions. To give a brief description of how clusters are defined, let's first define that for instances we will use $i$, and for clusters we will use $j$. Each term in the explanation is:

- $i$: Instance i^{th};
- $j$: Cluster j^{th};
- $x^{(i)}$: Location of instance $i;$
- $\Phi _{j}$: Weight, or probability that instance $i$ belongs to cluster $j$;
- $z^{(i)}$: Index of the cluster chosen for the $i^{th}$ instance
- $\mu ^{(j)}$: Mean of the Gaussian distribution;
- $\Sigma ^{(j)}$: Covariance of the matrix;

For the GMM to be performed, it is assumed that the dataset **X** is generated by the following process:

- A cluster k is randomly assigned to each instance, with probability of choosing the $j^{th}$ cluster from $\Phi _{j}$. The index of the cluster chosen for the $i^{th}$ instance will be $z^{(i)}$.

- If the instance $i^{th}$ is assigned to the cluster $j^{th}$ ($z^{(i)}$ = $j$), and the location $x^{(i)}$ of the instance is represented by $x^{(i)} ~ \nu(\mu ^{(j)}, \Sigma ^{(j)})$

Then, initially we estimate the weights $\Phi$ and the distribution parameters $\mu ^{(1)}$ to $\mu ^{(k)}$ and $\Sigma ^{(1)}$ to $\Sigma ^{(k)}$, as shown by the code below using the Scikit-Learn `GaussisanMixture` class for generating 3 clusters:



```python

from sklearn.mixture import GaussianMixture

gm = GaussianMixture(n_components=3, n_init=10)
gm.fit(X)

gm.weights_
gm.means_
gm.covariances_
```

The results of the means and covariance matrices obtained by the algorithm are very close to the real values. The class operates from the **Expectation Maximization** (EM) algorithm, which works similarly to _k-means_ as follows:

- Step 1: Initialize cluster with random parameters;
- Step 2: Assign instances to clusters (_Expectation Step_);
- Step 3: Update Clusters (_Maximization Step_);
- Step 4: Repeat steps 2 and 3.

>[!NOTE]
> What is the difference between **EM** and _K-Means_? The generalization of K-Means only finds the cluster centers ($\mu$), while EM also finds their size, shape and orientation ($\Sigma$) and also their relative weights ($\Phi$). While _K-Mean_ uses **Hard Cluster Attribution**, **EM** uses **Soft Cluster Attribution**.


During the _Expectation Stage_ the algorithm estimates the probability of the instance belonging to a cluster, and during the _Maximization Stage_ the clusters are updated using all instances of the dataset with estimated probabilities that each instance belongs to each cluster. This probability is called **Responsibilities of the Clusters for the instances**. Therefore, in each update, each cluster will have more impact from the instance that is most responsible for it.

Due to the possibility of converging to local minima, it is necessary to run the algorithm more than once. It is possible to analyze whether or not there was a conversion using the methods `converged_`, which returns a boolean, and `n_iter_`, which returns the number of iterations required for convergence.


**GMM** is a _Generative Model_, and it is possible to create new instances from the data available. It is also possible to output **Hard Clustering** or **Soft Clustering**:

- **Hard Clustering**: Assigns each instance to the most likely cluster, using the `predict()` method;
- **Soft Clustering**: Estimates probabilities of belonging to clusters, using the `predict_proba()` method.

  
<!---------------------------------------------------->
                              
## Using Gaussian Mixtures for Anomaly Detection


<!---------------------------------------------------->

## Selecting the Number of Clusters


<!---------------------------------------------------->
 
## Bayesian Gaussian Mixture Models 


<!---------------------------------------------------->

## Other Algorithms for Anomaly and Novelty Detection 



<!---------------------------------------------------->
