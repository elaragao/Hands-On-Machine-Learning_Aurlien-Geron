# Unsupervised Learning Techniques

Although most of the efforts made for Machine Learning are in supervised learning, most of the data that exists does not have captions, which highlights the importance of **unsupervised learning**. The following topics will be covered in the following chapter:

- **Clustering**: Groups similar instances into _Clusters_. It is useful for data analysis, customer segmentation, recommendation systems, search systems, image sizing, etc.

- **Anomaly Detection (or Outlier Detection)**: Its objective is to identify what is "normal" for the system and detect abnormal instances. Instances considered normal are called **inliers** and those that are abnormal are called **anomalies** or **outliers**. Widely used in fraud detection, defective products, time series analysis, etc.

- **Density Estimation**: Estimates the Probability Density Function (PDF) of what generated the data set. Used for anomaly detection such as instances in very low density regions, and also useful for data visualization.
  
<!---------------------------------------------------->
<!---------------------------------------------------->
<!---------------------------------------------------->


# Clustering Algorithms: k-means and DBSCAN 

**Clustering** consists of grouping instances within a group with other similar instances, called a **Cluster**. It is an unsupervised process, different from classification. Some of the applications of **Clustering** are:

- **Customer Segmentation**: Useful for _Recommender Systems_, when identifying patterns of purchases and activities on websites.

- **Data Analysis**: Helps in visualization;

- **Dimensionality Reduction**: After clustering, it is possible to see the _Affinity_ of the instances, which measures how well an instance is in a cluster.

- **Feature Engineering**: As shown in chapter 2, the geographic cluster of the California Housing dataset.

- **Anomaly Detection**: Detects based on the _Low Affinity_ of the instances in the clusters.

- **Semi-Supervised Learning**: If there are some instances, it is possible to cluster and propagate the captions to other instances in the same cluster.

- **Search Engines**: An example is image searches, finding one that is close to the cluster of the image used to perform the search.

- **Image Segmentation**: Clusters pixels according to colors, replacing and reducing the number of different colors, used for object detection and tracking systems, in addition to detecting contours.

There is no single way to define a cluster, it depends on the context and the algorithm. Some search for the center around a defined point called _Centroid_, others search for denser regions. Two algorithms for this called **k-means** and **DBSCAN** will be discussed shortly.
<!---------------------------------------------------->
<!---------------------------------------------------->
<!---------------------------------------------------->


## k-means 

The **k-means** algorithm is simple and capable of clustering certain data sets quickly, efficiently and in a few iterations. The algorithm is also known as the Lloyd-Forgy algorithm.


[Image]


The Scikit-Learn library through the `KMeans` class. The algorithm requires that you specify the number of clusters that will be analyzed (in general, this is not a trivial task, but it will be discussed below). For each instance that was identified as belonging to a cluster, a label will be assigned, corresponding to the index. Both the indices can be seen through the `labels_instance` variable, and the centroids through `cluster_centers_`.


```python
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
X, y = make_blobs([...]) # make the blobs: y contains the cluster IDs, but we
# will not use them; that's what we want to predict

k = 5
kmeans = KMeans(n_clusters=k, random_state=42)
y_pred = kmeans.fit_predict(X)

y_pred
y_pred is kmeans.labels_

kmeans.cluster_centers_
```


>[!CAUTION]
> The labels generated by the clustering algorithm are **different** from the labels in classification classes, which are used as **targets**. Clustering is an **Unsupervised** learning activity.


The algorithm does not perform as well when the clusters have different diameters, since it operates through the distance between the instance and the centroid. This is observable in some points identified as in the pink cluster, which would belong to the yellow cluster, as in the image below:

[Image]

In this algorithm, there are two main ways to classify the instances:
- **Hard Clustering**: Allocates the instance in the cluster closest to the centroid;
- **Soft Clustering**: Assigns scores to each instance in relation to each cluster. This score can be in relation to the instance and centroid, or in relation to the **Similarity Score** or **Affinity* (for example, Gaussina Radial Basis Function).
<!---------------------------------------------------->

### The k-means algorithm

In a considerable number of cases, it is not certain how many clusters there are. The algorithm can be operated by randomly placing the centroids in k locations, for example, in k random instances. Then, the instances are labeled and the centroids are updated, and this is done repeatedly. The image below demonstrates this starting in the top left corner, followed by the top right corner where the instances are labeled.

[Image]

>[!NOTE]
> The algorithm converges in a small number of iterations, usually small, since the mean squared distance of the instances and centroids decreases at each step and converges.

Although convergence is certain, it may not converge to the correct solution, but rather to a _local optimum_, depending on the initialization of the centroid.


<!---------------------------------------------------->

### Centroid initialization methods

One way to initialize the centroids in a more optimized way is to use the data found in the random initialization. This can be done as in the code below:

```python
good_init = np.array([[-3, 3], [-3, 2], [-3, 1], [-1, 2], [0, 2]])
kmeans = KMeans(n_clusters=5, init=good_init, n_init=1, random_state=42)
kmeans.fit(X)
```

Another possible solution is to run the algorithm several times and keep the best solution. This can be done using the hyperparameter `n_init` as any integer number you want.



> [!IMPORTANT]
> The way to identify which solution is the best is through the _Performance Metric_ called **Inertia**, which is the sum of the distances calculated between the instances and the closest centroid. The model maintained is the one with the lowest **Inertia**. It is observable through `kmeans.inertia_`. It is worth noting that the `score` method is its negative.


Recently, an improvement was made to the algorithm, called **k-means++**. It adds a computational step that drastically reduces the number of iterations that need to be done to find the optimal solution, and reduces the chance of falling into local minima or suboptimal solutions. It chooses to select centroids that are **Far** from each other. The `KMeans` class already uses this method.

The algorithm operates in the steps of, first, selecting the centroid $c^{(1)}$ in the dataset. After that, select a new centroid $c^{(i)}$ by choosing an instance $x^{(1)}$ through the probability expressed by the equation $D (x^{(i)})^{2} / \sum^{m}_{j=1}D (x^{(j)})^{2}$, where $D (x^{(i)})$ is the distance between the instance $x^{(i)}$ and the closest centroid already chosen (in the case in question, it would be $c^{(1)}$) and repeat this step until selecting the k centroids.



<!---------------------------------------------------->
### Accelerated k-means and mini-batch k-means

The **Elkan Algorithm** or **Accelerated Algorithm** is used in data sets with many clusters, avoiding unnecessary distance calculations. It does this through the triangle inequality and controlling lower and upper bounds for the distances between instances and centroids. It is used through the hyperparameter `algorithm = "elkan"`.

The use of **mini-batches** proposed by Schulley uses mini-batches by moving centroids only a little at each iteration, speeding up the algorithm and making it possible to cluster large data sets. It can be used with the `MiniBatchKMeans` class



<!---------------------------------------------------->
<!---------------------------------------------------->
## Limits of k-means


<!---------------------------------------------------->
                           
## Using Clustering for Image Segmentation 



<!---------------------------------------------------->

## Using Clustering for Semi-Supervised Learning  


<!---------------------------------------------------->

## DBSCAN 


<!---------------------------------------------------->
                                        
## Other Clustering Algorithms   



<!---------------------------------------------------->
<!---------------------------------------------------->
<!---------------------------------------------------->


# Gaussian Mixtures


<!---------------------------------------------------->
                              
## Using Gaussian Mixtures for Anomaly Detection


<!---------------------------------------------------->

## Selecting the Number of Clusters


<!---------------------------------------------------->
 
## Bayesian Gaussian Mixture Models 


<!---------------------------------------------------->

## Other Algorithms for Anomaly and Novelty Detection 



<!---------------------------------------------------->
