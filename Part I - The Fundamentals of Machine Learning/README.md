# Unsupervised Learning Techniques

Although most of the efforts made for Machine Learning are in supervised learning, most of the data that exists does not have captions, which highlights the importance of **unsupervised learning**. The following topics will be covered in the following chapter:

- **Clustering**: Groups similar instances into _Clusters_. It is useful for data analysis, customer segmentation, recommendation systems, search systems, image sizing, etc.

- **Anomaly Detection (or Outlier Detection)**: Its objective is to identify what is "normal" for the system and detect abnormal instances. Instances considered normal are called **inliers** and those that are abnormal are called **anomalies** or **outliers**. Widely used in fraud detection, defective products, time series analysis, etc.

- **Density Estimation**: Estimates the Probability Density Function (PDF) of what generated the data set. Used for anomaly detection such as instances in very low density regions, and also useful for data visualization.
  
<!---------------------------------------------------->
<!---------------------------------------------------->
<!---------------------------------------------------->


# Clustering Algorithms: k-means and DBSCAN 

**Clustering** consists of grouping instances within a group with other similar instances, called a **Cluster**. It is an unsupervised process, different from classification. Some of the applications of **Clustering** are:

- **Customer Segmentation**: Useful for _Recommender Systems_, when identifying patterns of purchases and activities on websites.

- **Data Analysis**: Helps in visualization;

- **Dimensionality Reduction**: After clustering, it is possible to see the _Affinity_ of the instances, which measures how well an instance is in a cluster.

- **Feature Engineering**: As shown in chapter 2, the geographic cluster of the California Housing dataset.

- **Anomaly Detection**: Detects based on the _Low Affinity_ of the instances in the clusters.

- **Semi-Supervised Learning**: If there are some instances, it is possible to cluster and propagate the captions to other instances in the same cluster.

- **Search Engines**: An example is image searches, finding one that is close to the cluster of the image used to perform the search.

- **Image Segmentation**: Clusters pixels according to colors, replacing and reducing the number of different colors, used for object detection and tracking systems, in addition to detecting contours.

There is no single way to define a cluster, it depends on the context and the algorithm. Some search for the center around a defined point called _Centroid_, others search for denser regions. Two algorithms for this called **k-means** and **DBSCAN** will be discussed shortly.
<!---------------------------------------------------->
<!---------------------------------------------------->
<!---------------------------------------------------->


## k-means 

The **k-means** algorithm is simple and capable of clustering certain data sets quickly, efficiently and in a few iterations. The algorithm is also known as the Lloyd-Forgy algorithm.


[Image]


The Scikit-Learn library through the `KMeans` class. The algorithm requires that you specify the number of clusters that will be analyzed (in general, this is not a trivial task, but it will be discussed below). For each instance that was identified as belonging to a cluster, a label will be assigned, corresponding to the index. Both the indices can be seen through the `labels_instance` variable, and the centroids through `cluster_centers_`.


```python
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
X, y = make_blobs([...]) # make the blobs: y contains the cluster IDs, but we
# will not use them; that's what we want to predict

k = 5
kmeans = KMeans(n_clusters=k, random_state=42)
y_pred = kmeans.fit_predict(X)

y_pred
y_pred is kmeans.labels_

kmeans.cluster_centers_
```


>[!CAUTION]
> The labels generated by the clustering algorithm are **different** from the labels in classification classes, which are used as **targets**. Clustering is an **Unsupervised** learning activity.


The algorithm does not perform as well when the clusters have different diameters, since it operates through the distance between the instance and the centroid. This is observable in some points identified as in the pink cluster, which would belong to the yellow cluster, as in the image below:

[Image]

In this algorithm, there are two main ways to classify the instances:
- **Hard Clustering**: Allocates the instance in the cluster closest to the centroid;
- **Soft Clustering**: Assigns scores to each instance in relation to each cluster. This score can be in relation to the instance and centroid, or in relation to the **Similarity Score** or **Affinity* (for example, Gaussina Radial Basis Function).
<!---------------------------------------------------->
              
## Limits of k-means


<!---------------------------------------------------->
                           
## Using Clustering for Image Segmentation 



<!---------------------------------------------------->

## Using Clustering for Semi-Supervised Learning  


<!---------------------------------------------------->

## DBSCAN 


<!---------------------------------------------------->
                                        
## Other Clustering Algorithms   



<!---------------------------------------------------->
<!---------------------------------------------------->
<!---------------------------------------------------->


# Gaussian Mixtures


<!---------------------------------------------------->
                              
## Using Gaussian Mixtures for Anomaly Detection


<!---------------------------------------------------->

## Selecting the Number of Clusters


<!---------------------------------------------------->
 
## Bayesian Gaussian Mixture Models 


<!---------------------------------------------------->

## Other Algorithms for Anomaly and Novelty Detection 



<!---------------------------------------------------->
